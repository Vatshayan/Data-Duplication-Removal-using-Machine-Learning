# Data-Duplication-Removal-using-Checksum-Project

"Deletion of Duplicated files so to Increase the space of system. Process will be done through checksum" 

Project is based on Machine Learning
![A Salute to Our Veterans](https://user-images.githubusercontent.com/28294942/139524192-b4379891-963b-4c8a-914f-b726a24009cb.png)


## Abstract: 
In computing, data deduplication is a technique for eliminating duplicate copies of repeating data. A related and somewhat synonymous term is single-instance (data) storage. This technique is used to improve storage utilization and can also be applied to network data transfers to reduce the number of bytes that must be sent. In the de-duplication process, unique chunks of data, or byte patterns, are identified and stored during a process of analysis. As the analysis continues, other chunks are compared to the stored copy and whenever a match occurs, the redundant chunk is replaced with a small reference that points to the stored chunk. Given that the same byte pattern may occur dozens, hundreds, or even thousands of times (the match frequency is dependent on the chunk size), the amount of data that must be stored or transferred can be greatly reduced. One of the most common forms of data de-duplication implementations works by comparing chunks of data to detect duplicates. For that to happen, each chunk of data is assigned an identification, calculated by the software, typically using cryptographic hash functions. In many implementations, the assumption is made that if the identification is identical, the data is identical, even though this cannot be true in all cases due to the pigeonhole principle; other implementations do not assume that two blocks of data with the same identifier are identical, but actually verify that data with the same identification is identical. If the software either assumes that a given identification already exists in the de-duplication namespace or actually verifies the identity of the two blocks of data, depending on the implementation, then it will replace that duplicate chunk with a link.

## Some Points:

-	Detecting and removing duplicates using Machine Learning by calculating the digest of files which takes less time than other pre-implemented methods. 

-	The project proposes an efficient method for detecting and removing duplicates using machine learning algorithms.

-	Storage optimization by de-duplication.

-	We read the file with duplicate data and store all the unique entries in it in another new file.

-	The input dataset consists of a lot of duplicate entries.

- The goal of this Project is to use a machine learning approach to remove those duplicate entries.

## Want Project files ? 

**You Can use this Beautiful Project for your college Project and get good marks too.**

Email me Now **vatshayan007@gmail.com** to get this Full Project Code, PPT, Report, Synopsis, Video Presentation and Research paper of this Project.

ðŸ’Œ Feel free to contact me for any kind of help on any projects.

### Need Code, Documents & Explanation video ? 

## How to Reach me :

### Mail : vatshayan007@gmail.com 

### WhatsApp: **+91 9310631437** (Helping 24*7) **[CHAT](https://wa.me/message/CHWN2AHCPMAZK1)** 

### Website : https://www.finalproject.in/

### 1000 Computer Science Projects : https://www.computer-science-project.in/

### New CSE Project : [Face Mask Detection Project](https://github.com/Vatshayan/Face-Mask-Detection-Project)

